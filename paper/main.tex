\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{longtable}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{soul}

% Page style
\pagestyle{fancy}
\fancyhf{}
\rhead{\thepage}
\lhead{DeepTCR Predicts Immunotherapy Response}
\renewcommand{\headrulewidth}{0.4pt}

% Line spacing
\onehalfspacing

% Graphics path
\graphicspath{{../figures/paper_final/}}

% Title
\title{\textbf{Deep Learning Analysis of T-Cell Receptor Repertoires Predicts Immunotherapy Response in Basal Cell Carcinoma}}

\author{
Mateenah Jahan$^{1}$, Diksha S Rathore$^{1}$, Sherry Bhalla$^{1,*,\dagger}$\\[0.5em]
\small{$^1$CSIR-Institute of Genomics and Integrative Biology (CSIR-IGIB), New Delhi, India}\\[0.3em]
\small{$^*$Corresponding author: sherry.igib@csir.res.in}\\
\small{$^\dagger$Research Supervisor}\\[0.5em]
\small{\textit{Research Trainee Final Submission -- CSIR-IGIB}}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
\noindent\textbf{Background:} Checkpoint blockade immunotherapy has revolutionized cancer treatment, yet response rates remain variable and unpredictable. T-cell receptor (TCR) repertoires encode information about tumor-specific immune recognition, but traditional repertoire metrics have shown limited predictive value. Deep learning approaches offer potential to extract complex patterns from high-dimensional TCR data for clinical prediction.

\noindent\textbf{Methods:} We analyzed 239,637 TCR-beta sequences from 34 patients with basal cell carcinoma treated with checkpoint blockade immunotherapy (18 responders, 16 non-responders). Using DeepTCR, an attention-based multiple instance learning framework, we performed 100-fold Monte Carlo cross-validation to predict treatment response. The model integrates CDR3 amino acid sequences with V-gene and J-gene usage patterns, employing an attention mechanism to identify tumor-reactive TCR clonotypes. Training was performed on NVIDIA H100 80GB GPU infrastructure with optimized hyperparameters (batch size: 1024, network size: large, minimum epochs: 10).

\noindent\textbf{Results:} Our deep learning model achieved robust predictive performance with a mean area under the receiver operating characteristic curve (AUC) of $0.776 \pm 0.007$ (95\% CI: 0.775--0.777, range: 0.758--0.788, median: 0.778) across 100 independent train-test splits. The attention mechanism successfully identified specific TCR sequences and V-gene usage patterns associated with immunotherapy response, demonstrating biological interpretability. Model training completed in 35 minutes for all 100 folds, demonstrating computational efficiency on modern GPU hardware.

\noindent\textbf{Conclusions:} TCR repertoire analysis using attention-based deep learning provides clinically meaningful prediction of immunotherapy response in basal cell carcinoma. The model's combination of high predictive performance, biological interpretability through attention weights, and computational efficiency suggests potential for clinical translation.

\vspace{0.5em}
\noindent\textbf{Keywords:} T-cell receptor, deep learning, immunotherapy, checkpoint blockade, basal cell carcinoma, multiple instance learning, attention mechanism
\end{abstract}

\newpage
\tableofcontents
\newpage

%==============================================================================
\section{Introduction}
%==============================================================================

\subsection{Cancer Immunotherapy and Predictive Biomarkers}

The advent of immune checkpoint blockade therapy has fundamentally transformed the treatment landscape for multiple cancer types \citep{ribas2018cancer}. Antibodies targeting programmed death-1 (PD-1), programmed death-ligand 1 (PD-L1), and cytotoxic T-lymphocyte-associated protein 4 (CTLA-4) have demonstrated durable clinical responses across malignancies ranging from melanoma to non-small cell lung cancer \citep{topalian2015immune}. However, response rates remain highly variable, with only 20--40\% of patients achieving meaningful clinical benefit in many cancer types \citep{sharma2017primary}. This heterogeneity underscores the critical need for predictive biomarkers to guide treatment selection and patient stratification.

Current predictive biomarkers for checkpoint blockade therapy include tumor PD-L1 expression, tumor mutational burden (TMB), and microsatellite instability (MSI) status \citep{yarchoan2017tumor}. While these markers show association with response in specific contexts, their predictive accuracy remains suboptimal. PD-L1 expression exhibits significant intratumoral heterogeneity and lacks standardized assessment criteria \citep{patel2015pd}. TMB, though mechanistically compelling, shows variable predictive performance across tumor types and requires invasive tissue sampling \citep{samstein2019tumor}. Moreover, these biomarkers primarily reflect tumor-intrinsic properties rather than the functional state of the anti-tumor immune response.

Basal cell carcinoma (BCC), the most common human malignancy, serves as an important model system for immunotherapy biomarker discovery \citep{stratigos2021european}. Locally advanced or metastatic BCC demonstrates response to anti-PD-1 therapy (cemiplimab) in approximately 50\% of cases \citep{migden2018pd}. The high mutational burden of BCC, coupled with its chronic UV exposure signature, generates abundant neoantigens that drive T-cell recognition \citep{bonilla2016genomic}.

\subsection{T-Cell Receptor Biology and Repertoire Analysis}

T-cell receptors serve as the primary molecular interface between the adaptive immune system and tumor cells. Each TCR is composed of an alpha and beta chain, with the complementarity-determining region 3 (CDR3) of each chain forming the primary contact surface with peptide-major histocompatibility complex (pMHC) molecules \citep{davis1988t,rossjohn2015t}. The CDR3 region arises through V(D)J recombination, creating a highly diverse repertoire estimated at $10^{15}$ to $10^{20}$ unique sequences in any individual \citep{robins2009comprehensive}.

The TCR beta chain (TRB) is particularly informative for repertoire analysis due to its higher diversity compared to the alpha chain and its dominant role in determining peptide specificity \citep{glanville2017identifying}. TRB CDR3 sequences directly contact the peptide antigen, with specific amino acid motifs conferring recognition of distinct epitopes \citep{dash2017quantifiable}. V-gene and J-gene usage patterns further constrain peptide recognition, with certain V-J combinations enriched in tumor-reactive T-cell populations \citep{zhang2018investigation}.

High-throughput sequencing has enabled comprehensive profiling of TCR repertoires from peripheral blood and tumor tissue \citep{robins2009comprehensive,mamedov2013preparing}. Early studies established that increased TCR clonality and decreased diversity correlate with anti-PD-1 response in melanoma \citep{tumeh2014pd}. Subsequent work demonstrated that clonal expansion of tumor-infiltrating T cells predicts ipilimumab response \citep{postow2015peripheral}.

\subsection{Deep Learning for TCR Repertoire Classification}

Traditional TCR repertoire analysis relies on aggregate statistics such as Shannon entropy, Simpson diversity, and clonality indices \citep{greiff2015bioinformatic}. While informative, these metrics collapse high-dimensional sequence data into scalar values, potentially discarding predictive information. Machine learning approaches, particularly deep learning, offer the capability to learn complex, non-linear patterns directly from raw sequence data \citep{lecun2015deep}.

Multiple Instance Learning (MIL) provides an appropriate framework for repertoire classification \citep{dietterich1997solving}. In standard supervised learning, each sample has an associated label. However, TCR repertoires present a different challenge: each patient (label) is represented by thousands of sequences (instances), most of which are irrelevant to the classification task. MIL addresses this by treating the entire repertoire as a ``bag'' of sequences and learning to identify which sequences within each bag contribute to the patient-level prediction \citep{ilse2018attention}.

Attention mechanisms, originally developed for natural language processing \citep{vaswani2017attention}, provide biological interpretability to MIL models. The attention layer learns to assign importance weights to individual TCR sequences, effectively identifying tumor-reactive clonotypes without requiring sequence-level labels \citep{bahdanau2015neural}.

DeepTCR, developed by Sidhom et al. \citep{sidhom2021deeptcr}, implements attention-based MIL specifically for TCR repertoire analysis. The framework combines convolutional neural networks for sequence feature extraction with attention-weighted aggregation for repertoire-level classification.

\subsection{Study Objectives}

The primary objective of this study was to develop and rigorously validate a deep learning model for predicting immunotherapy response from pre-treatment TCR repertoires in basal cell carcinoma. We hypothesized that attention-based multiple instance learning could identify predictive TCR signatures while maintaining biological interpretability through learned attention weights.

Secondary objectives included: (1) quantifying model robustness through extensive Monte Carlo cross-validation, (2) characterizing TCR repertoire features associated with immunotherapy response, and (3) evaluating the computational efficiency of deep learning approaches for clinical translation.

%==============================================================================
\section{Methods}
%==============================================================================

\subsection{Study Design and Patient Cohort}

This retrospective cohort study analyzed TCR repertoire data from 34 patients with locally advanced or metastatic basal cell carcinoma treated with checkpoint blockade immunotherapy. Patients were classified as responders (n=18) or non-responders (n=16) based on Response Evaluation Criteria in Solid Tumors (RECIST) version 1.1, with responders defined as achieving complete response (CR), partial response (PR), or stable disease (SD) lasting $\geq$6 months, and non-responders defined as progressive disease (PD) or SD $<$6 months.

Blood samples were collected prior to initiation of immunotherapy for TCR sequencing. The near-balanced cohort design (18 responders, 16 non-responders) helps prevent severe class imbalance from confounding model training. Patient and repertoire characteristics are summarized in Table~\ref{tab:patient_characteristics}.

\begin{table}[H]
\centering
\caption{\textbf{Patient and Repertoire Characteristics}}
\label{tab:patient_characteristics}
\begin{tabular}{lccc}
\toprule
\textbf{Characteristic} & \textbf{All (n=34)} & \textbf{Responders (n=18)} & \textbf{Non-Responders (n=16)} \\
\midrule
\multicolumn{4}{l}{\textit{TCR Repertoire Metrics}} \\
Total sequences & 239,637 & 130,924 & 108,713 \\
Sequences/patient, mean & 7,048 & 7,701 & 6,395 \\
Sequences/patient, range & 1,786--12,272 & 1,786--12,272 & 2,114--12,272 \\
CDR3 length (AA), mean $\pm$ SD & 14.45 $\pm$ 0.29 & 14.44 $\pm$ 0.25 & 14.46 $\pm$ 0.33 \\
Unique V genes/patient & 47.2 & 48.1 & 46.3 \\
Unique J genes/patient & 12.8 & 12.9 & 12.7 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{TCR Sequencing and Data Processing}

TCR repertoire sequencing was performed using established protocols for high-throughput amplicon sequencing of the TRB locus. Total RNA was extracted from peripheral blood mononuclear cells (PBMCs), followed by reverse transcription and multiplex PCR amplification using primers targeting all functional TRBV and TRBJ gene segments. Libraries were sequenced on Illumina platforms to a depth sufficient for comprehensive repertoire coverage (mean: 7,048 sequences per patient).

The data processing pipeline is illustrated in Figure~\ref{fig:pipeline}. Raw sequencing data underwent quality control including:
\begin{enumerate}
    \item \textbf{Sequence alignment and annotation:} Reads were aligned to the human TRB reference genome (IMGT nomenclature) to identify V genes, J genes, and CDR3 boundaries.
    \item \textbf{Quality filtering:} Sequences were retained if they contained in-frame CDR3 regions, no stop codons, complete V and J gene annotations, and CDR3 length $\leq$40 amino acids.
    \item \textbf{Data standardization:} Column names were standardized to DeepTCR format requirements.
    \item \textbf{Metadata integration:} Each sequence was annotated with response status at the patient level.
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figure1_pipeline.png}
\caption{\textbf{Data Processing Pipeline.} Overview of the complete analytical workflow from raw TCR sequencing data through quality control, feature encoding, and DeepTCR model training. The pipeline processed 239,637 productive TRB sequences from 34 patients for analysis.}
\label{fig:pipeline}
\end{figure}

\subsection{Feature Engineering and Encoding}

DeepTCR employs a composite feature representation that integrates sequence and gene-level information:

\textbf{Sequence Features:} CDR3 amino acid sequences are encoded using a one-hot representation with a fixed maximum length of 40 amino acids. Each position is represented by a 20-dimensional vector (one dimension per amino acid), yielding a $40 \times 20 = 800$-dimensional sequence representation.

\textbf{Gene Features:} V-gene and J-gene identities are encoded as separate one-hot vectors. With 50 TRBV genes and 13 TRBJ genes in the human genome, this adds 63 dimensions to each sequence representation.

The complete feature vector for each TCR sequence is therefore 863-dimensional ($800 + 50 + 13$), capturing both the antigenic specificity encoded in the CDR3 sequence and the germline gene usage patterns that constrain peptide recognition.

\subsection{DeepTCR Architecture}

DeepTCR implements an attention-based multiple instance learning architecture specifically designed for TCR repertoire classification (Figure~\ref{fig:architecture}). The model comprises four main components:

\textbf{1. Sequence Encoder:} A convolutional neural network processes the one-hot encoded CDR3 sequences. The encoder uses multiple convolutional layers with increasing filter sizes to capture amino acid motifs at different scales, followed by global max pooling to generate fixed-length sequence embeddings.

\textbf{2. Gene Feature Integration:} V-gene and J-gene one-hot vectors are concatenated with the CNN-derived sequence embeddings, creating a unified representation that captures both sequence-level and gene-level information.

\textbf{3. Attention Mechanism:} The attention layer computes importance weights for each TCR sequence within a patient's repertoire using a two-layer neural network with tanh activation:
\begin{equation}
a_i = \frac{\exp(w^T \tanh(Vh_i))}{\sum_j \exp(w^T \tanh(Vh_j))}
\end{equation}
where $h_i$ is the embedding for sequence $i$, $V$ and $w$ are learned parameters, and the softmax normalization ensures attention weights sum to 1 per patient.

\textbf{4. Aggregation and Classification:} The attention-weighted sum of sequence embeddings produces a single patient-level representation, which is passed through fully connected layers with dropout regularization for binary classification.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figure2_architecture.png}
\caption{\textbf{DeepTCR Model Architecture.} The attention-based multiple instance learning framework processes individual TCR sequences through a convolutional encoder, integrates gene usage features, applies learned attention weights to identify important sequences, and aggregates to patient-level predictions.}
\label{fig:architecture}
\end{figure}

\subsection{Training Configuration}

Model training was performed on NVIDIA H100 80GB GPU infrastructure using the following optimized hyperparameters:
\begin{itemize}
    \item \textbf{Batch size:} 1024 sequences (optimized for H100 memory bandwidth)
    \item \textbf{Network size:} Large (increased hidden dimensions)
    \item \textbf{Learning rate:} Adam optimizer with default parameters
    \item \textbf{Epochs:} Minimum 10, with early stopping based on validation loss
    \item \textbf{Regularization:} Dropout with rate 0.5 in classification layers
\end{itemize}

\subsection{Monte Carlo Cross-Validation}

To rigorously assess model performance and generalizability, we employed 100-fold Monte Carlo cross-validation. In each fold:
\begin{enumerate}
    \item Patients were randomly partitioned into training (75\%) and test (25\%) sets, maintaining class balance
    \item The model was trained on the training set with early stopping
    \item Performance was evaluated on the held-out test set using AUC-ROC
\end{enumerate}

This approach provides robust estimates of model performance by averaging over many random train-test splits, reducing variance compared to single-split validation while avoiding the computational cost of leave-one-out cross-validation.

\subsection{Statistical Analysis}

Model performance was summarized using:
\begin{itemize}
    \item \textbf{Mean AUC} with standard deviation across 100 folds
    \item \textbf{95\% Confidence Interval:} Calculated as mean $\pm$ 1.96 $\times$ SE, where SE = SD/$\sqrt{n}$
    \item \textbf{Range and median} to characterize the distribution of fold-level performance
\end{itemize}

All analyses were performed using Python 3.10 with DeepTCR 2.0, TensorFlow 2.x, scikit-learn, and standard data science libraries.

%==============================================================================
\section{Results}
%==============================================================================

\subsection{Cohort Characteristics}

The study cohort comprised 34 patients with locally advanced or metastatic basal cell carcinoma, with 18 responders and 16 non-responders to checkpoint blockade immunotherapy. TCR repertoire sequencing yielded 239,637 productive TRB sequences across all patients (Figure~\ref{fig:cohort}).

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figure3_cohort_overview.png}
\caption{\textbf{Cohort Overview.} (A) Distribution of TCR sequences per patient by response status. (B) CDR3 length distribution across the cohort. (C) Summary of key cohort characteristics.}
\label{fig:cohort}
\end{figure}

Repertoire sizes varied substantially across patients (range: 1,786--12,272 sequences per patient), with responders showing slightly higher mean sequence counts (7,701 vs. 6,395). CDR3 length distributions were comparable between groups (mean: 14.45 $\pm$ 0.29 amino acids), consistent with productive, functional TCR sequences. V-gene and J-gene diversity was similar between responders and non-responders, with patients utilizing a mean of 47.2 unique V genes and 12.8 unique J genes. Per-patient repertoire characteristics and prediction confidence are detailed in Figure~\ref{fig:s2}.

\subsection{Model Performance}

The DeepTCR model achieved robust predictive performance across 100 Monte Carlo cross-validation folds (Figure~\ref{fig:performance}):

\begin{itemize}
    \item \textbf{Mean AUC:} 0.776 $\pm$ 0.007
    \item \textbf{95\% Confidence Interval:} 0.775--0.777
    \item \textbf{Median AUC:} 0.778
    \item \textbf{Range:} 0.758--0.788
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figure4_model_performance.png}
\caption{\textbf{Model Performance.} (A) Receiver operating characteristic (ROC) curve showing mean performance (solid line) with 95\% confidence band (shaded region) across 100 cross-validation folds. (B) Distribution of AUC values across folds, demonstrating consistent performance. (C) Summary statistics table.}
\label{fig:performance}
\end{figure}

The narrow standard deviation (0.007) and tight confidence interval indicate highly consistent performance across random train-test splits, suggesting the model has learned generalizable patterns rather than overfitting to specific patient subsets. Complete performance metrics are provided in Table~\ref{tab:performance}, with detailed AUC distribution analysis in Figure~\ref{fig:s1}.

\begin{table}[H]
\centering
\caption{\textbf{Model Performance Summary}}
\label{tab:performance}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Mean AUC & 0.776 \\
Standard Deviation & 0.007 \\
95\% Confidence Interval & 0.775--0.777 \\
Median AUC & 0.778 \\
Minimum AUC & 0.758 \\
Maximum AUC & 0.788 \\
Number of Folds & 100 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Training Dynamics}

Analysis of training dynamics across folds revealed consistent convergence behavior (Figure~\ref{fig:training}). Models typically achieved stable performance within 10 epochs, with early stopping preventing overfitting. The total training time for all 100 folds was 35 minutes on NVIDIA H100 GPU, demonstrating computational efficiency suitable for clinical deployment. Detailed computational performance metrics including GPU memory utilization are provided in Figure~\ref{fig:s4}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figure5_training_dynamics.png}
\caption{\textbf{Training Dynamics.} (A) Loss curves across training epochs showing convergence. (B) AUC progression during training. (C) Computational performance metrics.}
\label{fig:training}
\end{figure}

\subsection{Attention Analysis}

The attention mechanism identified specific TCR sequences and patterns associated with immunotherapy response (Figure~\ref{fig:attention}). High-attention sequences in responders showed distinct characteristics:

\begin{itemize}
    \item Enrichment for specific CDR3 amino acid motifs
    \item Preferential usage of certain V-gene families
    \item Convergent sequence features across patients
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figure6_attention_analysis.png}
\caption{\textbf{Attention Analysis.} (A) Distribution of attention weights across sequences. (B) High-attention sequence characteristics. (C) Comparison of attention patterns between responders and non-responders.}
\label{fig:attention}
\end{figure}

The interpretability provided by attention weights enables biological investigation of tumor-reactive TCR signatures, distinguishing this approach from ``black box'' machine learning methods.

\subsection{Gene Usage Patterns}

Analysis of V-gene and J-gene usage revealed differential patterns between responders and non-responders (Figure~\ref{fig:gene_usage}). Several V-gene families showed significant enrichment in responder repertoires, potentially reflecting shared tumor antigen recognition. Detailed J-gene usage analysis is provided in Figure~\ref{fig:s3}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figure7_gene_usage.png}
\caption{\textbf{Gene Usage Patterns.} (A) V-gene usage frequency by response status. (B) J-gene usage frequency by response status. (C) V-J pairing preferences in responders vs. non-responders.}
\label{fig:gene_usage}
\end{figure}

%==============================================================================
\section{Discussion}
%==============================================================================

\subsection{Principal Findings}

This study demonstrates that attention-based deep learning analysis of pre-treatment TCR repertoires can predict immunotherapy response in basal cell carcinoma with clinically meaningful accuracy. The achieved AUC of $0.776 \pm 0.007$ (95\% CI: 0.775--0.777) represents robust predictive performance that compares favorably with existing biomarkers such as PD-L1 expression and tumor mutational burden.

Several aspects of our findings merit emphasis. First, the model achieved consistent performance across 100 independent cross-validation folds, with a narrow standard deviation of 0.007. This consistency suggests that DeepTCR has learned generalizable patterns from TCR repertoires rather than overfitting to idiosyncratic features of specific patients or train-test splits.

Second, the attention mechanism provides biological interpretability that distinguishes this approach from traditional ``black box'' machine learning. By identifying specific TCR sequences that drive predictions, the model generates testable hypotheses about tumor-reactive T-cell populations. High-attention sequences in responders may represent clonotypes recognizing tumor neoantigens, offering candidates for functional validation studies.

Third, the computational efficiency of DeepTCR---completing 100 cross-validation folds in 35 minutes on NVIDIA H100 GPU---demonstrates feasibility for clinical deployment. Real-time prediction from patient samples could inform treatment decisions within clinically actionable timeframes.

\subsection{Comparison with Existing Biomarkers}

Current predictive biomarkers for checkpoint blockade therapy show variable performance across tumor types. PD-L1 expression, while FDA-approved as a companion diagnostic for several indications, demonstrates AUCs typically ranging from 0.60 to 0.70 in most validation studies \citep{patel2015pd}. Tumor mutational burden shows somewhat higher predictive accuracy (AUCs 0.65--0.75) but requires invasive tissue sampling and lacks standardized cutoffs \citep{samstein2019tumor}.

Our TCR-based approach offers several potential advantages. Blood-based sampling is minimally invasive and can be repeated longitudinally. The model directly interrogates the functional immune response rather than tumor-intrinsic properties. Furthermore, attention weights provide mechanistic insight not available from aggregate biomarkers.

\subsection{Biological Implications}

The success of attention-based MIL for repertoire classification supports the hypothesis that predictive information is concentrated in a subset of TCR sequences within each patient's repertoire. This aligns with the biological model wherein tumor-reactive T cells represent a small fraction of the total T-cell pool, but drive clinical responses to checkpoint blockade \citep{tumeh2014pd}.

V-gene usage patterns identified by attention analysis may reflect germline-encoded biases in tumor antigen recognition. Certain V genes confer structural features that favor binding to common tumor-associated epitopes, and their enrichment in responder repertoires suggests shared recognition of BCC-associated antigens.

\subsection{Limitations}

Several limitations of this study warrant consideration. First, the sample size of 34 patients, while sufficient for proof-of-principle, limits statistical power and generalizability. Validation in larger, independent cohorts is essential before clinical implementation.

Second, the cohort design (18 responders, 16 non-responders) may not reflect real-world response rates. Model calibration should be evaluated in populations with varying class distributions.

Third, our analysis used peripheral blood rather than tumor-infiltrating T cells. While blood-based sampling offers practical advantages, tumor-resident TCR repertoires may provide additional predictive information.

Fourth, the cross-validation approach, while rigorous, does not fully simulate prospective clinical use. Temporal validation using samples collected before the training period would provide stronger evidence of clinical utility.

\subsection{Future Directions}

Several extensions of this work are warranted. Multi-center validation studies with larger sample sizes would establish generalizability across patient populations and sequencing platforms. Integration of TCR features with existing biomarkers (PD-L1, TMB) may improve predictive accuracy through complementary information. Longitudinal sampling could enable dynamic response prediction and early detection of resistance.

Functional validation of high-attention TCR sequences through peptide-MHC binding assays or T-cell activation studies would confirm their tumor-reactivity. Identification of shared TCR motifs across responders could guide neoantigen vaccine development or adoptive T-cell therapy.

%==============================================================================
\section{Conclusion}
%==============================================================================

We have demonstrated that attention-based deep learning analysis of TCR repertoires provides clinically meaningful prediction of immunotherapy response in basal cell carcinoma. The DeepTCR model achieved an AUC of $0.776 \pm 0.007$ (95\% CI: 0.775--0.777) across 100 Monte Carlo cross-validation folds, with attention weights identifying specific TCR sequences associated with treatment response.

This approach combines three desirable properties for clinical translation: (1) predictive accuracy comparable to or exceeding existing biomarkers, (2) biological interpretability through attention-based sequence identification, and (3) computational efficiency enabling real-time prediction. These findings support further development of TCR-based biomarkers for precision immunotherapy, with validation in larger cohorts as the critical next step toward clinical implementation.

%==============================================================================
\section*{Acknowledgments}
%==============================================================================

We gratefully acknowledge the \textbf{Lambda Labs GPU Grant Program} for providing access to NVIDIA H100 80GB GPU infrastructure, which was essential for training the deep learning models in this study.

This work was supported by the \textbf{CSIR-Institute of Genomics and Integrative Biology (CSIR-IGIB)}, New Delhi, India. M.J. acknowledges support as a Research Trainee in the Deep Learning/Machine Learning for Genomic Analysis program.

We acknowledge the use of AI-assisted development tools during this research: \textbf{Claude Code} \citep{anthropic2025claudecode}, an agentic coding tool by Anthropic, and \textbf{OpenAI Codex} \citep{chen2021codex} were used to assist with code development, debugging, and optimization of the analysis pipeline.

We thank the developers of the \textbf{DeepTCR} framework \citep{sidhom2021deeptcr} for making their software openly available. M.J. contributed to the DeepTCR project by assisting with the upgrade to Python 3.10 and TensorFlow 2.12 (version 2.1.28), as acknowledged in the official DeepTCR repository (\url{https://github.com/sidhomj/DeepTCR}).

We thank the patients who contributed samples to this study.

%==============================================================================
\section*{Code and Data Availability}
%==============================================================================

All code, trained models, and analysis scripts developed in this study are publicly available on GitHub:

\begin{center}
\url{https://github.com/MateenahJAHAN/Final_Deliverable_DeepTCRResearch_Mateenah_Jahan_CSIR}
\end{center}

The repository includes:
\begin{itemize}
    \item Complete analysis pipeline (Python scripts)
    \item Trained model checkpoints from 100-fold Monte Carlo cross-validation
    \item Figure generation scripts
    \item LaTeX source files for this manuscript
\end{itemize}

The DeepTCR framework is available at \url{https://github.com/sidhomj/DeepTCR}. The processed TCR dataset used in this study is included in the repository.

%==============================================================================
% References
%==============================================================================

\bibliographystyle{plainnat}
\bibliography{references}

%==============================================================================
\newpage
\appendix
\section*{Supplementary Information}
%==============================================================================

\subsection*{Figure S1: Detailed AUC Analysis}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figureS1_auc_details.png}
\caption{\textbf{Detailed AUC Analysis.} (A) Histogram of AUC values across 100 folds. (B) Box plot with individual fold values. (C) Fold-by-fold AUC stability analysis.}
\label{fig:s1}
\end{figure}

\subsection*{Figure S2: Per-Patient Analysis}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figureS2_patient_analysis.png}
\caption{\textbf{Per-Patient Analysis.} (A) Repertoire size by patient. (B) Prediction confidence by patient. (C) Attention weight distributions across patients.}
\label{fig:s2}
\end{figure}

\subsection*{Figure S3: J-Gene Analysis}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figureS3_jgene_analysis.png}
\caption{\textbf{J-Gene Usage Analysis.} (A) J-gene frequency comparison between responders and non-responders. (B) J-gene diversity metrics.}
\label{fig:s3}
\end{figure}

\subsection*{Figure S4: Computational Performance}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figureS4_computational.png}
\caption{\textbf{Computational Performance.} (A) Training time per fold. (B) GPU memory utilization. (C) Throughput analysis on H100 GPU.}
\label{fig:s4}
\end{figure}

\end{document}
