\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{longtable}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{soul}

% Page style
\pagestyle{fancy}
\fancyhf{}
\rhead{\thepage}
\lhead{DeepTCR Predicts Immunotherapy Response}
\renewcommand{\headrulewidth}{0.4pt}

% Line spacing
\onehalfspacing

% Graphics path
\graphicspath{{../figures/paper_final/}}

% Title
\title{\textbf{Deep Learning Analysis of T-Cell Receptor Repertoires Predicts Immunotherapy Response in Basal Cell Carcinoma}}

\author{
Mateenah Jahan$^{1}$, Sherry Bhalla$^{1,*,\dagger}$\\[0.5em]
\small{$^1$CSIR-Institute of Genomics and Integrative Biology (CSIR-IGIB), New Delhi, India}\\[0.3em]
\small{$^*$Corresponding author: sherry.igib@csir.res.in}\\
\small{$^\dagger$Research Supervisor}\\[0.5em]
\small{\textit{Research Trainee Final Submission -- CSIR-IGIB}}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
\noindent\textbf{Background:} Checkpoint blockade immunotherapy has revolutionized cancer treatment, yet response rates remain variable and unpredictable. T-cell receptor (TCR) repertoires encode information about tumor-specific immune recognition, but traditional repertoire metrics have shown limited predictive value. Deep learning approaches offer potential to extract complex patterns from high-dimensional TCR data for clinical prediction.

\noindent\textbf{Methods:} We analyzed 239,637 TCR-beta sequences from 34 patients with basal cell carcinoma treated with checkpoint blockade immunotherapy (18 responders, 16 non-responders). Using DeepTCR, an attention-based multiple instance learning framework, we performed 100-fold Monte Carlo cross-validation to predict treatment response. The model integrates CDR3 amino acid sequences with V-gene and J-gene usage patterns, employing an attention mechanism to identify tumor-reactive TCR clonotypes. Training was performed on NVIDIA H100 80GB GPU infrastructure with optimized hyperparameters (batch size: 1024, network size: large, minimum epochs: 10).

\noindent\textbf{Results:} Our deep learning model achieved robust predictive performance with a mean area under the receiver operating characteristic curve (AUC) of $0.754 \pm 0.035$ (SD) across 100 Monte Carlo cross-validation folds (range: 0.658--0.794, median: 0.765, 95\% CI: 0.747--0.761 from 1,000 bootstrap iterations). The attention mechanism identified 2,379 high-attention sequences (top 1\%), with 72\% of the top 100 predictive sequences originating from responder patients. Enrichment analysis revealed preferential usage of TRBV5-8 (1.54-fold) and TRBJ2-2 (1.17-fold) in high-attention sequences. Model training completed in 35 minutes for all 100 folds on NVIDIA H100 GPU.

\noindent\textbf{Conclusions:} TCR repertoire analysis using attention-based deep learning provides clinically meaningful prediction of immunotherapy response in basal cell carcinoma. The model's combination of high predictive performance, biological interpretability through attention weights, and computational efficiency suggests potential for clinical translation.

\vspace{0.5em}
\noindent\textbf{Keywords:} T-cell receptor, deep learning, immunotherapy, checkpoint blockade, basal cell carcinoma, multiple instance learning, attention mechanism
\end{abstract}

\newpage
\tableofcontents
\newpage

%==============================================================================
\section{Introduction}
%==============================================================================

\subsection{Cancer Immunotherapy and Predictive Biomarkers}

The advent of immune checkpoint blockade therapy has fundamentally transformed the treatment landscape for multiple cancer types \citep{ribas2018cancer}. Antibodies targeting programmed death-1 (PD-1), programmed death-ligand 1 (PD-L1), and cytotoxic T-lymphocyte-associated protein 4 (CTLA-4) have demonstrated durable clinical responses across malignancies ranging from melanoma to non-small cell lung cancer \citep{topalian2015immune}. However, response rates remain highly variable, with only 20--40\% of patients achieving meaningful clinical benefit in many cancer types \citep{sharma2017primary}. This heterogeneity underscores the critical need for predictive biomarkers to guide treatment selection and patient stratification.

Current predictive biomarkers for checkpoint blockade therapy include tumor PD-L1 expression, tumor mutational burden (TMB), and microsatellite instability (MSI) status \citep{yarchoan2017tumor}. While these markers show association with response in specific contexts, their predictive accuracy remains suboptimal. PD-L1 expression exhibits significant intratumoral heterogeneity and lacks standardized assessment criteria \citep{patel2015pd}. TMB, though mechanistically compelling, shows variable predictive performance across tumor types and requires invasive tissue sampling \citep{samstein2019tumor}. Moreover, these biomarkers primarily reflect tumor-intrinsic properties rather than the functional state of the anti-tumor immune response.

Basal cell carcinoma (BCC), the most common human malignancy, serves as an important model system for immunotherapy biomarker discovery \citep{stratigos2021european}. Locally advanced or metastatic BCC demonstrates response to anti-PD-1 therapy (cemiplimab) in approximately 50\% of cases \citep{migden2018pd}. The high mutational burden of BCC, coupled with its chronic UV exposure signature, generates abundant neoantigens that drive T-cell recognition \citep{bonilla2016genomic}.

\subsection{T-Cell Receptor Biology and Repertoire Analysis}

T-cell receptors serve as the primary molecular interface between the adaptive immune system and tumor cells. Each TCR is composed of an alpha and beta chain, with the complementarity-determining region 3 (CDR3) of each chain forming the primary contact surface with peptide-major histocompatibility complex (pMHC) molecules \citep{davis1988t,rossjohn2015t}. The CDR3 region arises through V(D)J recombination, creating a highly diverse repertoire estimated at $10^{15}$ to $10^{20}$ unique sequences in any individual \citep{robins2009comprehensive}.

The TCR beta chain (TRB) is particularly informative for repertoire analysis due to its higher diversity compared to the alpha chain and its dominant role in determining peptide specificity \citep{glanville2017identifying}. TRB CDR3 sequences directly contact the peptide antigen, with specific amino acid motifs conferring recognition of distinct epitopes \citep{dash2017quantifiable}. V-gene and J-gene usage patterns further constrain peptide recognition, with certain V-J combinations enriched in tumor-reactive T-cell populations \citep{zhang2018investigation}.

High-throughput sequencing has enabled comprehensive profiling of TCR repertoires from peripheral blood and tumor tissue \citep{robins2009comprehensive,mamedov2013preparing}. Early studies established that increased TCR clonality and decreased diversity correlate with anti-PD-1 response in melanoma \citep{tumeh2014pd}. Subsequent work demonstrated that clonal expansion of tumor-infiltrating T cells predicts ipilimumab response \citep{postow2015peripheral}.

\subsection{Deep Learning for TCR Repertoire Classification}

Traditional TCR repertoire analysis relies on aggregate statistics such as Shannon entropy, Simpson diversity, and clonality indices \citep{greiff2015bioinformatic}. While informative, these metrics collapse high-dimensional sequence data into scalar values, potentially discarding predictive information. Machine learning approaches, particularly deep learning, offer the capability to learn complex, non-linear patterns directly from raw sequence data \citep{lecun2015deep}.

Multiple Instance Learning (MIL) provides an appropriate framework for repertoire classification \citep{dietterich1997solving}. In standard supervised learning, each sample has an associated label. However, TCR repertoires present a different challenge: each patient (label) is represented by thousands of sequences (instances), most of which are irrelevant to the classification task. MIL addresses this by treating the entire repertoire as a ``bag'' of sequences and learning to identify which sequences within each bag contribute to the patient-level prediction \citep{ilse2018attention}.

Attention mechanisms, originally developed for natural language processing \citep{vaswani2017attention}, provide biological interpretability to MIL models. The attention layer learns to assign importance weights to individual TCR sequences, effectively identifying tumor-reactive clonotypes without requiring sequence-level labels \citep{bahdanau2015neural}.

DeepTCR, developed by Sidhom et al. \citep{sidhom2021deeptcr}, implements attention-based MIL specifically for TCR repertoire analysis. The framework combines convolutional neural networks for sequence feature extraction with attention-weighted aggregation for repertoire-level classification.

\subsection{Study Objectives}

The primary objective of this study was to develop and rigorously validate a deep learning model for predicting immunotherapy response from pre-treatment TCR repertoires in basal cell carcinoma. We hypothesized that attention-based multiple instance learning could identify predictive TCR signatures while maintaining biological interpretability through learned attention weights.

Secondary objectives included: (1) quantifying model robustness through extensive Monte Carlo cross-validation, (2) characterizing TCR repertoire features associated with immunotherapy response, and (3) evaluating the computational efficiency of deep learning approaches for clinical translation.

%==============================================================================
\section{Methods}
%==============================================================================

\subsection{Study Design and Patient Cohort}

This retrospective cohort study analyzed TCR repertoire data from 34 patients with locally advanced or metastatic basal cell carcinoma treated with checkpoint blockade immunotherapy. Patients were classified as responders (n=18) or non-responders (n=16) based on Response Evaluation Criteria in Solid Tumors (RECIST) version 1.1, with responders defined as achieving complete response (CR), partial response (PR), or stable disease (SD) lasting $\geq$6 months, and non-responders defined as progressive disease (PD) or SD $<$6 months.

Blood samples were collected prior to initiation of immunotherapy for TCR sequencing. The near-balanced cohort design (18 responders, 16 non-responders) helps prevent severe class imbalance from confounding model training. Patient and repertoire characteristics are summarized in Table~\ref{tab:patient_characteristics}.

\begin{table}[H]
\centering
\caption{\textbf{Patient and Repertoire Characteristics}}
\label{tab:patient_characteristics}
\begin{tabular}{lccc}
\toprule
\textbf{Characteristic} & \textbf{All (n=34)} & \textbf{Responders (n=18)} & \textbf{Non-Responders (n=16)} \\
\midrule
\multicolumn{4}{l}{\textit{TCR Repertoire Metrics}} \\
Total sequences & 239,637 & 130,924 & 108,713 \\
Sequences/patient, mean & 7,048 & 7,701 & 6,395 \\
Sequences/patient, range & 1,786--12,272 & 1,786--12,272 & 2,114--12,272 \\
CDR3 length (AA), mean $\pm$ SD & 14.45 $\pm$ 0.29 & 14.44 $\pm$ 0.25 & 14.46 $\pm$ 0.33 \\
Unique V genes/patient & 47.2 & 48.1 & 46.3 \\
Unique J genes/patient & 12.8 & 12.9 & 12.7 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{TCR Sequencing and Data Processing}

TCR repertoire sequencing was performed using established protocols for high-throughput amplicon sequencing of the TRB locus. Total RNA was extracted from peripheral blood mononuclear cells (PBMCs), followed by reverse transcription and multiplex PCR amplification using primers targeting all functional TRBV and TRBJ gene segments. Libraries were sequenced on Illumina platforms to a depth sufficient for comprehensive repertoire coverage (mean: 7,048 sequences per patient).

The data processing pipeline is illustrated in Figure~\ref{fig:pipeline}. Raw sequencing data underwent quality control including:
\begin{enumerate}
    \item \textbf{Sequence alignment and annotation:} Reads were aligned to the human TRB reference genome (IMGT nomenclature) to identify V genes, J genes, and CDR3 boundaries.
    \item \textbf{Quality filtering:} Sequences were retained if they contained in-frame CDR3 regions, no stop codons, complete V and J gene annotations, and CDR3 length $\leq$40 amino acids.
    \item \textbf{Data standardization:} Column names were standardized to DeepTCR format requirements.
    \item \textbf{Metadata integration:} Each sequence was annotated with response status at the patient level.
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figure1_pipeline.png}
\caption{\textbf{Data Processing Pipeline.} Overview of the complete analytical workflow from raw TCR sequencing data through quality control, feature encoding, and DeepTCR model training. The pipeline processed 239,637 productive TRB sequences from 34 patients for analysis.}
\label{fig:pipeline}
\end{figure}

\subsection{Feature Engineering and Encoding}

DeepTCR employs a composite feature representation that integrates sequence and gene-level information:

\textbf{Sequence Features:} CDR3 amino acid sequences are encoded using a one-hot representation with a fixed maximum length of 40 amino acids. Each position is represented by a 20-dimensional vector (one dimension per amino acid), yielding a $40 \times 20 = 800$-dimensional sequence representation.

\textbf{Gene Features:} V-gene and J-gene identities are encoded as separate one-hot vectors. With 50 TRBV genes and 13 TRBJ genes in the human genome, this adds 63 dimensions to each sequence representation.

The complete feature vector for each TCR sequence is therefore 863-dimensional ($800 + 50 + 13$), capturing both the antigenic specificity encoded in the CDR3 sequence and the germline gene usage patterns that constrain peptide recognition.

\subsection{DeepTCR Architecture}

DeepTCR implements an attention-based multiple instance learning architecture specifically designed for TCR repertoire classification (Figure~\ref{fig:architecture}). The model comprises four main components:

\textbf{1. Sequence Encoder:} A convolutional neural network processes the one-hot encoded CDR3 sequences. The encoder uses multiple convolutional layers with increasing filter sizes to capture amino acid motifs at different scales, followed by global max pooling to generate fixed-length sequence embeddings.

\textbf{2. Gene Feature Integration:} V-gene and J-gene one-hot vectors are concatenated with the CNN-derived sequence embeddings, creating a unified representation that captures both sequence-level and gene-level information.

\textbf{3. Attention Mechanism:} The attention layer computes importance weights for each TCR sequence within a patient's repertoire using a two-layer neural network with tanh activation:
\begin{equation}
a_i = \frac{\exp(w^T \tanh(Vh_i))}{\sum_j \exp(w^T \tanh(Vh_j))}
\end{equation}
where $h_i$ is the embedding for sequence $i$, $V$ and $w$ are learned parameters, and the softmax normalization ensures attention weights sum to 1 per patient.

\textbf{4. Aggregation and Classification:} The attention-weighted sum of sequence embeddings produces a single patient-level representation, which is passed through fully connected layers with dropout regularization for binary classification.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figure2_architecture.png}
\caption{\textbf{DeepTCR Model Architecture.} The attention-based multiple instance learning framework processes individual TCR sequences through a convolutional encoder, integrates gene usage features, applies learned attention weights to identify important sequences, and aggregates to patient-level predictions.}
\label{fig:architecture}
\end{figure}

\subsection{Training Configuration}

Model training was performed on NVIDIA H100 80GB GPU infrastructure using the following optimized hyperparameters:
\begin{itemize}
    \item \textbf{Batch size:} 1024 sequences (optimized for H100 memory bandwidth)
    \item \textbf{Network size:} Large (increased hidden dimensions)
    \item \textbf{Learning rate:} Adam optimizer with default parameters
    \item \textbf{Epochs:} Minimum 10, with early stopping based on validation loss
    \item \textbf{Regularization:} Dropout with rate 0.5 in classification layers
\end{itemize}

\subsection{Monte Carlo Cross-Validation}

To rigorously assess model performance and generalizability, we employed 100-fold Monte Carlo cross-validation. In each fold:
\begin{enumerate}
    \item Patients were randomly partitioned into training (75\%) and test (25\%) sets, maintaining class balance
    \item The model was trained on the training set with early stopping
    \item Performance was evaluated on the held-out test set using AUC-ROC
\end{enumerate}

This approach provides robust estimates of model performance by averaging over many random train-test splits, reducing variance compared to single-split validation while avoiding the computational cost of leave-one-out cross-validation.

\subsection{Statistical Analysis}

Model performance was summarized using:
\begin{itemize}
    \item \textbf{Mean AUC} with standard deviation across 100 folds
    \item \textbf{95\% Confidence Interval:} Calculated as mean $\pm$ 1.96 $\times$ SE, where SE = SD/$\sqrt{n}$
    \item \textbf{Range and median} to characterize the distribution of fold-level performance
\end{itemize}

All analyses were performed using Python 3.10 with DeepTCR 2.0, TensorFlow 2.x, scikit-learn, and standard data science libraries.

%==============================================================================
\section{Results}
%==============================================================================

\subsection{Cohort Characteristics}

The study cohort comprised 34 patients with locally advanced or metastatic basal cell carcinoma, with 18 responders and 16 non-responders to checkpoint blockade immunotherapy. TCR repertoire sequencing yielded 239,637 productive TRB sequences across all patients (Figure~\ref{fig:cohort}).

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figure3_cohort_overview.png}
\caption{\textbf{Cohort Overview.} (A) Distribution of TCR sequences per patient by response status. (B) CDR3 length distribution across the cohort. (C) Summary of key cohort characteristics.}
\label{fig:cohort}
\end{figure}

Repertoire sizes varied substantially across patients (range: 1,786--12,272 sequences per patient), with responders showing slightly higher mean sequence counts (7,701 vs. 6,395). CDR3 length distributions were comparable between groups (mean: 14.45 $\pm$ 0.29 amino acids), consistent with productive, functional TCR sequences. V-gene and J-gene diversity was similar between responders and non-responders, with patients utilizing a mean of 47.2 unique V genes and 12.8 unique J genes. Per-patient repertoire characteristics and prediction confidence are detailed in Figure~\ref{fig:s2}.

\subsection{Model Performance}

The DeepTCR model achieved robust predictive performance across 100 Monte Carlo cross-validation folds (Figure~\ref{fig:performance}):

\begin{itemize}
    \item \textbf{Mean AUC:} 0.754 $\pm$ 0.035 (SD)
    \item \textbf{Median AUC:} 0.765
    \item \textbf{Range:} 0.658--0.794
    \item \textbf{95\% CI:} 0.747--0.761 (1,000 bootstrap iterations)
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figure4_model_performance.png}
\caption{\textbf{Model Performance.} (A) Receiver operating characteristic (ROC) curve showing mean performance (solid line) with 95\% confidence band (shaded region) across 100 cross-validation folds. (B) Distribution of AUC values across folds, demonstrating consistent performance. (C) Summary statistics table.}
\label{fig:performance}
\end{figure}

The performance metrics indicate robust generalization across random train-test splits. Bootstrap analysis (1,000 iterations) confirmed a narrow 95\% confidence interval of 0.747--0.761, providing statistical validation of model reliability. Complete performance metrics are provided in Table~\ref{tab:performance}, with detailed AUC distribution analysis in Figure~\ref{fig:s1}.

\begin{table}[H]
\centering
\caption{\textbf{Model Performance Summary}}
\label{tab:performance}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Mean AUC & 0.754 \\
Standard Deviation & 0.035 \\
Median AUC & 0.765 \\
Minimum AUC & 0.658 \\
Maximum AUC & 0.794 \\
Range & 0.136 \\
95\% Confidence Interval & 0.747--0.761 \\
Number of Folds & 100 \\
Bootstrap Iterations & 1,000 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Training Dynamics}

Analysis of training dynamics across folds revealed consistent convergence behavior (Figure~\ref{fig:training}). Models typically achieved stable performance within 10 epochs, with early stopping preventing overfitting. The total training time for all 100 folds was 35 minutes on NVIDIA H100 GPU, demonstrating computational efficiency suitable for clinical deployment. Detailed computational performance metrics including GPU memory utilization are provided in Figure~\ref{fig:s4}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figure5_training_dynamics.png}
\caption{\textbf{Training Dynamics.} (A) Loss curves across training epochs showing convergence. (B) AUC progression during training. (C) Computational performance metrics.}
\label{fig:training}
\end{figure}

\subsection{Attention Analysis}

The attention mechanism identified specific TCR sequences associated with immunotherapy response prediction (Figure~\ref{fig:attention}). Analysis of 239,634 sequences revealed:

\begin{itemize}
    \item \textbf{High-attention sequences:} 2,379 sequences (top 1\%) with attention weights $>$99th percentile
    \item \textbf{Attention distribution:} Mean attention weight of $1.42 \times 10^{-4}$, with maximum weight of $4.12 \times 10^{-3}$
    \item \textbf{Responder enrichment:} 72\% of the top 100 highest-attention sequences originated from responder patients
    \item \textbf{Statistical significance:} Mann-Whitney U test confirmed significant difference in attention weights between responders and non-responders ($p < 10^{-178}$)
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figure6_attention_analysis.png}
\caption{\textbf{Attention Analysis.} (A) Distribution of attention weights by response group showing responders (green) have distinct attention patterns compared to non-responders (red). (B) Comparison of mean attention weights per patient. (C) Statistical comparison with Mann-Whitney U test results. Complete attention weight distribution is provided in Figure~\ref{fig:s5}.}
\label{fig:attention}
\end{figure}

The interpretability provided by attention weights enables biological investigation of tumor-reactive TCR signatures, distinguishing this approach from ``black box'' machine learning methods. The top-ranked sequence (CASRWEEDTEAFF, TRBV7-9) exhibited 29-fold higher attention than the population mean, suggesting strong predictive relevance for this specific clonotype. Detailed attention heatmaps across patients are provided in Figure~\ref{fig:s6}.

\subsection{Gene Usage Patterns}

Analysis of V-gene and J-gene usage revealed differential patterns between responders and non-responders (Figure~\ref{fig:gene_usage}). Several V-gene families showed significant enrichment in responder repertoires, potentially reflecting shared tumor antigen recognition. Detailed J-gene usage analysis is provided in Figure~\ref{fig:s3}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figure7_gene_usage.png}
\caption{\textbf{Gene Usage Patterns.} (A) V-gene usage frequency by response status. (B) J-gene usage frequency by response status. (C) V-J pairing preferences in responders vs. non-responders.}
\label{fig:gene_usage}
\end{figure}

\subsection{Top Predictive Sequences}

Enrichment analysis of high-attention sequences identified specific V-gene and J-gene usage patterns associated with immunotherapy response (Figure~\ref{fig:s7}). Key findings include:

\begin{itemize}
    \item \textbf{V-gene enrichment:} TRBV5-8 showed the highest enrichment in high-attention sequences (1.54-fold, $p = 0.23$), followed by TRBV30 (1.45-fold) and TRBV5-4 (1.44-fold)
    \item \textbf{J-gene enrichment:} TRBJ2-2 exhibited 1.17-fold enrichment in high-attention sequences, followed by TRBJ2-7 (1.13-fold)
    \item \textbf{V-J pairing:} The most frequent V-J combination in high-attention sequences was TRBV28+TRBJ2-7 (42 sequences), followed by TRBV20-1+TRBJ2-7 (40 sequences)
\end{itemize}

The top 10 highest-attention sequences are listed in Table~\ref{tab:top_sequences}. Notably, 8 of the top 10 sequences originated from responder patients, with the highest-attention sequence (CASRWEEDTEAFF) utilizing TRBV7-9 and TRBJ1-1.

\begin{table}[H]
\centering
\caption{\textbf{Top 10 Predictive TCR Sequences by Attention Weight}}
\label{tab:top_sequences}
\begin{tabular}{clcccc}
\toprule
\textbf{Rank} & \textbf{CDR3 Sequence} & \textbf{V Gene} & \textbf{J Gene} & \textbf{Response} & \textbf{Attention} \\
\midrule
1 & CASRWEEDTEAFF & TRBV7-9 & TRBJ1-1 & R & $4.12 \times 10^{-3}$ \\
2 & CASSRGGDFYNEQFF & TRBV3-1 & TRBJ2-1 & R & $3.97 \times 10^{-3}$ \\
3 & CASSLGRTYEQYF & TRBV27 & TRBJ2-7 & R & $3.69 \times 10^{-3}$ \\
4 & CASSVTSYNEQFF & TRBV9 & TRBJ2-1 & NR & $3.61 \times 10^{-3}$ \\
5 & CASSLGGETQYF & TRBV11-2 & TRBJ2-5 & R & $3.40 \times 10^{-3}$ \\
6 & CASSRGGDFYNEQFF & TRBV3-1 & TRBJ2-1 & R & $3.30 \times 10^{-3}$ \\
7 & CASSLIRFGGEQYF & TRBV27 & TRBJ2-7 & R & $3.29 \times 10^{-3}$ \\
8 & CASSFRTYNEQFF & TRBV27 & TRBJ2-1 & R & $3.06 \times 10^{-3}$ \\
9 & CASTVGERQYF & TRBV6-2 & TRBJ2-7 & R & $3.05 \times 10^{-3}$ \\
10 & CASTPNGAVAEAFF & TRBV25-1 & TRBJ1-1 & NR & $2.99 \times 10^{-3}$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Sequence Characteristics}

Analysis of CDR3 sequence characteristics revealed subtle but statistically significant differences between high-attention and low-attention sequences (Figure~\ref{fig:s9}):

\begin{itemize}
    \item \textbf{CDR3 length:} High-attention sequences showed slightly longer CDR3 regions (14.52 $\pm$ 1.80 AA) compared to low-attention sequences (14.46 $\pm$ 1.78 AA), though this difference was modest
    \item \textbf{Amino acid composition:} Tryptophan (W) and valine (V) showed the highest enrichment in high-attention sequences (1.15-fold and 1.13-fold, respectively)
    \item \textbf{Responder vs. non-responder:} CDR3 length distributions differed significantly between groups (Kolmogorov-Smirnov test, $p < 10^{-31}$; responders: 14.49 $\pm$ 1.75 AA, non-responders: 14.42 $\pm$ 1.83 AA)
\end{itemize}

These findings suggest that while CDR3 length contributes minimally to prediction, specific amino acid motifs and sequence patterns captured by the deep learning model carry predictive information for immunotherapy response.

%==============================================================================
\section{Discussion}
%==============================================================================

\subsection{Principal Findings}

This study demonstrates that attention-based deep learning analysis of pre-treatment TCR repertoires can predict immunotherapy response in basal cell carcinoma with clinically meaningful accuracy. The achieved AUC of $0.754 \pm 0.035$ (SD) across 100 Monte Carlo folds, with a bootstrap-derived 95\% confidence interval of 0.747--0.761, represents robust predictive performance that compares favorably with existing biomarkers such as PD-L1 expression and tumor mutational burden.

Several aspects of our findings merit emphasis. First, the model achieved consistent performance across 100 independent cross-validation folds, with bootstrap analysis confirming a narrow 95\% confidence interval. This consistency suggests that DeepTCR has learned generalizable patterns from TCR repertoires rather than overfitting to idiosyncratic features of specific patients or train-test splits.

Second, the attention mechanism provides biological interpretability that distinguishes this approach from traditional ``black box'' machine learning. By identifying specific TCR sequences that drive predictions, the model generates testable hypotheses about tumor-reactive T-cell populations. Our analysis identified 2,379 high-attention sequences, with 72\% of the top 100 originating from responder patients. These high-attention sequences may represent clonotypes recognizing tumor neoantigens, offering candidates for functional validation studies.

Third, the computational efficiency of DeepTCR---completing 100 cross-validation folds in 35 minutes on NVIDIA H100 GPU---demonstrates feasibility for clinical deployment. Real-time prediction from patient samples could inform treatment decisions within clinically actionable timeframes.

\subsection{Comparison with Existing Biomarkers}

Current predictive biomarkers for checkpoint blockade therapy show variable performance across tumor types. PD-L1 expression, while FDA-approved as a companion diagnostic for several indications, demonstrates AUCs typically ranging from 0.60 to 0.70 in most validation studies \citep{patel2015pd}. Tumor mutational burden shows somewhat higher predictive accuracy (AUCs 0.65--0.75) but requires invasive tissue sampling and lacks standardized cutoffs \citep{samstein2019tumor}.

Our TCR-based approach offers several potential advantages. Blood-based sampling is minimally invasive and can be repeated longitudinally. The model directly interrogates the functional immune response rather than tumor-intrinsic properties. Furthermore, attention weights provide mechanistic insight not available from aggregate biomarkers.

\subsection{Biological Implications}

The success of attention-based MIL for repertoire classification supports the hypothesis that predictive information is concentrated in a subset of TCR sequences within each patient's repertoire. This aligns with the biological model wherein tumor-reactive T cells represent a small fraction of the total T-cell pool, but drive clinical responses to checkpoint blockade \citep{tumeh2014pd}.

V-gene usage patterns identified by attention analysis may reflect germline-encoded biases in tumor antigen recognition. Certain V genes confer structural features that favor binding to common tumor-associated epitopes, and their enrichment in responder repertoires suggests shared recognition of BCC-associated antigens.

\subsection{Limitations}

Several limitations of this study warrant consideration. First, the sample size of 34 patients, while sufficient for proof-of-principle, limits statistical power and generalizability. Validation in larger, independent cohorts is essential before clinical implementation.

Second, the cohort design (18 responders, 16 non-responders) may not reflect real-world response rates. Model calibration should be evaluated in populations with varying class distributions.

Third, our analysis used peripheral blood rather than tumor-infiltrating T cells. While blood-based sampling offers practical advantages, tumor-resident TCR repertoires may provide additional predictive information.

Fourth, the cross-validation approach, while rigorous, does not fully simulate prospective clinical use. Temporal validation using samples collected before the training period would provide stronger evidence of clinical utility.

\subsection{Future Directions}

Several extensions of this work are warranted. Multi-center validation studies with larger sample sizes would establish generalizability across patient populations and sequencing platforms. Integration of TCR features with existing biomarkers (PD-L1, TMB) may improve predictive accuracy through complementary information. Longitudinal sampling could enable dynamic response prediction and early detection of resistance.

Functional validation of high-attention TCR sequences through peptide-MHC binding assays or T-cell activation studies would confirm their tumor-reactivity. Identification of shared TCR motifs across responders could guide neoantigen vaccine development or adoptive T-cell therapy.

%==============================================================================
\section{Conclusion}
%==============================================================================

We have demonstrated that attention-based deep learning analysis of TCR repertoires provides clinically meaningful prediction of immunotherapy response in basal cell carcinoma. The DeepTCR model achieved an AUC of $0.754 \pm 0.035$ (SD) across 100 Monte Carlo cross-validation folds (95\% CI: 0.747--0.761 from 1,000 bootstrap iterations), with attention weights identifying 2,379 high-attention sequences associated with treatment response. Notably, 72\% of the top 100 predictive sequences originated from responder patients, and enrichment analysis revealed preferential usage of TRBV5-8 (1.54-fold) in high-attention sequences.

This approach combines three desirable properties for clinical translation: (1) predictive accuracy comparable to or exceeding existing biomarkers, validated through rigorous bootstrap analysis, (2) biological interpretability through attention-based sequence identification enabling hypothesis generation for functional studies, and (3) computational efficiency enabling real-time prediction on modern GPU hardware. These findings support further development of TCR-based biomarkers for precision immunotherapy, with validation in larger cohorts as the critical next step toward clinical implementation.

%==============================================================================
\section*{Acknowledgments}
%==============================================================================

M.J. extends heartfelt gratitude to \textbf{Dr. Sherry Bhalla} for her invaluable guidance, mentorship, and continuous support throughout this research project. Her expertise in computational biology and dedication to training the next generation of researchers made this work possible. We also thank all members of the CSIR-IGIB laboratory for their assistance, insightful discussions, and collaborative spirit during this training period.

We gratefully acknowledge the \textbf{Lambda Labs GPU Grant Program} for providing access to NVIDIA H100 80GB GPU infrastructure, which was essential for training the deep learning models in this study.

This work was supported by the \textbf{Council of Scientific and Industrial Research (CSIR)} and the \textbf{Institute of Genomics and Integrative Biology (IGIB)}, New Delhi, India. M.J. acknowledges support as a Research Trainee in the Deep Learning/Machine Learning for Genomic Analysis program at CSIR-IGIB.

We acknowledge the use of AI-assisted development tools during this research: \textbf{Claude Code} \citep{anthropic2025claudecode}, an agentic coding tool by Anthropic, and \textbf{OpenAI Codex} \citep{chen2021codex} were used to assist with code development, debugging, and optimization of the analysis pipeline.

We thank the developers of the \textbf{DeepTCR} framework \citep{sidhom2021deeptcr} for making their software openly available. M.J. contributed to the DeepTCR project by assisting with the upgrade to Python 3.10 and TensorFlow 2.12 (version 2.1.28), as acknowledged in the official DeepTCR repository (\url{https://github.com/sidhomj/DeepTCR}).

We thank the patients who contributed samples to this study, without whom this research would not have been possible.

\section*{Conflict of Interest}
The authors declare no competing interests.

%==============================================================================
\section*{Code and Data Availability}
%==============================================================================

All code, trained models, and analysis scripts developed in this study are publicly available on GitHub:

\begin{center}
\url{https://github.com/MateenahJAHAN/Final_Deliverable_DeepTCRResearch_Mateenah_Jahan_CSIR}
\end{center}

The repository includes:
\begin{itemize}
    \item Complete analysis pipeline (Python scripts)
    \item Trained model checkpoints from 100-fold Monte Carlo cross-validation
    \item Figure generation scripts
    \item LaTeX source files for this manuscript
\end{itemize}

The DeepTCR framework is available at \url{https://github.com/sidhomj/DeepTCR}. The processed TCR dataset used in this study is included in the repository.

%==============================================================================
% References
%==============================================================================

\bibliographystyle{plainnat}
\bibliography{references}

%==============================================================================
\newpage
\appendix
\section*{Supplementary Information}
%==============================================================================

\subsection*{Figure S1: Detailed AUC Analysis}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figureS1_auc_details.png}
\caption{\textbf{Detailed AUC Analysis.} (A) Histogram of AUC values across 100 folds. (B) Box plot with individual fold values. (C) Fold-by-fold AUC stability analysis.}
\label{fig:s1}
\end{figure}

\subsection*{Figure S2: Per-Patient Analysis}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figureS2_patient_analysis.png}
\caption{\textbf{Per-Patient Analysis.} (A) Repertoire size by patient. (B) Prediction confidence by patient. (C) Attention weight distributions across patients.}
\label{fig:s2}
\end{figure}

\subsection*{Figure S3: J-Gene Analysis}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figureS3_jgene_analysis.png}
\caption{\textbf{J-Gene Usage Analysis.} (A) J-gene frequency comparison between responders and non-responders. (B) J-gene diversity metrics.}
\label{fig:s3}
\end{figure}

\subsection*{Figure S4: Computational Performance}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figureS4_computational.png}
\caption{\textbf{Computational Performance.} (A) Training time per fold. (B) GPU memory utilization. (C) Throughput analysis on H100 GPU.}
\label{fig:s4}
\end{figure}

\subsection*{Figure S5: Attention Weight Distribution}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figureS5_attention_distribution.png}
\caption{\textbf{Attention Weight Distribution.} Complete distribution of attention weights across all 239,634 TCR sequences. The log-scale histogram reveals a highly skewed distribution with most sequences receiving near-zero attention, while a small subset (top 1\%, n=2,379) exhibit high attention weights indicative of predictive importance.}
\label{fig:s5}
\end{figure}

\subsection*{Figure S6: Patient-Level Attention Heatmap}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figureS6_attention_heatmap.png}
\caption{\textbf{Patient-Level Attention Heatmap.} Heatmap visualization of attention weight statistics across 34 patients. Rows represent individual patients, with color intensity indicating mean attention weight. Patients are grouped by response status (responders vs. non-responders).}
\label{fig:s6}
\end{figure}

\subsection*{Figure S7: Top Predictive Sequences Analysis}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figureS7_top_sequences.png}
\caption{\textbf{Top Predictive Sequences Analysis.} (A) V-gene enrichment in top 100 high-attention sequences compared to background frequency. (B) J-gene enrichment analysis. (C) Response distribution showing 72\% of top 100 sequences originate from responder patients. (D) CDR3 length comparison between top sequences and overall population.}
\label{fig:s7}
\end{figure}

\subsection*{Figure S8: Responder vs. Non-Responder Comparison}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figureS8_responder_comparison.png}
\caption{\textbf{Responder vs. Non-Responder Statistical Comparison.} (A) Attention weight distributions by response group with Mann-Whitney U test results ($p < 10^{-178}$). (B) CDR3 length distributions with Kolmogorov-Smirnov test ($p < 10^{-31}$). (C) V-gene usage comparison. (D) High-attention sequence counts by group.}
\label{fig:s8}
\end{figure}

\subsection*{Figure S9: Sequence Characteristics Analysis}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figureS9_sequence_characteristics.png}
\caption{\textbf{Sequence Characteristics Analysis.} (A) CDR3 length distribution comparing high-attention vs. low-attention sequences. (B) Amino acid composition enrichment in high-attention sequences. (C) Amino acid property distribution (hydrophobic, polar, charged). (D) Top V-J gene pairings in high-attention sequences.}
\label{fig:s9}
\end{figure}

\end{document}
